# -*- coding: utf-8 -*-
"""Face Image Motion Model (Photo-2-Video) Eng.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/tg-bomze/Face-Image-Motion-Model/blob/master/Face_Image_Motion_Model_(Photo_2_Video)_Eng.ipynb

<b><font color="black" size="+4">Face Image Motion Model</font></b>

<b><font color="black" size="+2">Based on:</font></b>

**GitHub repository**: [first-order-model](https://github.com/AliaksandrSiarohin/first-order-model)

Article: [First Order Motion Model for Image Animation](https://aliaksandrsiarohin.github.io/first-order-model-website/)

Creators: **[Aliaksandr Siarohin](https://github.com/AliaksandrSiarohin), [Stéphane Lathuilière](http://stelat.eu/), [Sergey Tulyakov](http://stulyakov.com/), [Elisa Ricci](http://elisaricci.eu/) and [Nicu Sebe](http://disi.unitn.it/~sebe/).**

<b><font color="black" size="+2">Put it all together:</font></b>

GitHub: [@tg-bomze](https://github.com/tg-bomze) & [@JamesCullum](https://github.com/JamesCullum),
Telegram: [@bomze](https://t.me/bomze),
Twitter: [@tg_bomze](https://twitter.com/tg_bomze).

---

To get started, click on the buttons (where the red arrow indicates) in each block in turn. After clicking, wait until the execution is complete.
"""

# Commented out IPython magic to ensure Python compatibility.
#@title <b><font color="red" size="+3">←</font><font color="black" size="+3"> Clone the repository and install all requirements</font></b>

# # %cd /content
# !rm -rf first_order_model
# !git clone https://github.com/AliaksandrSiarohin/first-order-model first_order_model
# !rm -rf sample_data
# # %cd first_order_model
# !mkdir frames

# if mirror == "Gofile.io":
#   !curl -A 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0' https://srv-file22.gofile.io/getUpload?c=SUyfwc
#   !wget -U "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0" https://srv-file22.gofile.io/download/SUyfwc/vox-cpk.pth.tar -O vox-cpk.pth.tar
#   !wget -U "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0" https://srv-file22.gofile.io/download/SUyfwc/shape_predictor_68_face_landmarks.dat.bz2 -O shape_predictor_68_face_landmarks.dat.bz2
#   !wget -U "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0" https://srv-file22.gofile.io/download/SUyfwc/align.zip -O align.zip
# elif mirror == "Google Drive":
#   !gdown https://drive.google.com/uc?id=1jvzD5ef6mPHRpISATDLqDMn90V-qNkV0
#   !gdown https://drive.google.com/uc?id=1idGJFxGDgbv25ZPFCX-faloByZbPu-it
#   !gdown https://drive.google.com/uc?id=1urTYmmckJPPXVKpkriqaUDb7InUi7AP3

# # provoke exception if file doesn't exist, so that it doesn't fail silently
# with open('align.zip') as testopen:
#   pass
# !unzip -u align.zip
# !pip install ffmpeg
# !pip install torchvision==0.5
# !pip install torch==1.4

import time
import imageio
import numpy as np
import torch
import glob
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from skimage.transform import resize
import io
import base64
import warnings
import os
import cv2
from demo import load_checkpoints
from demo import make_animation
from skimage import img_as_ubyte

#@title <b><font color="red" size="+3">←</font><font color="black" size="+3"> Upload a square video in mp4</font></b>

#@markdown **You can crop video here: https://ezgif.com/crop-video**

#@markdown **Convert video here: https://convert-video-online.com**

#@markdown ---


# video_capture = cv2.VideoCapture(0)

# frame_width = int(video_capture.get(3))
# frame_height = int(video_capture.get(4))

# size = (frame_width, frame_height)

# fourcc = cv2.VideoWriter_fourcc(*"MP4V")
# out = cv2.VideoWriter('output.mp4',fourcc, 20, size)

# cap = cv2.VideoCapture(0)
# # If your webcam does not support 640 x 480, this will find another resolution
# # cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
# # cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
# while True:
#     # Read the a frame from webcam
#     _, frame = cap.read()
#     # Flip the frame
#     frame = cv2.flip(frame, 1)
#     out.write(frame)
#     cv2.imshow("Landmarks detection using dlib", frame)

#     if cv2.waitKey(1) == ord('q'):
#         break

# cap.release()
# cv2.destroyAllWindows()

# time.sleep(1)
# vid = 'C:\\Users\\mahta\\OneDrive\\Documents\\GitHub\\autism_Robot\\src\\deep_fake\\first-order-model-master\\first-order-model-master\\08.mp4'
vid = "C:\\Users\\mahta\\OneDrive\\Documents\\GitHub\\autism_Robot\\src\\deep_fake\\first-order-model-master\\first-order-model-master\\04.mp4"

fps_of_video = int(cv2.VideoCapture(vid).get(cv2.CAP_PROP_FPS))
frames_of_video = int(cv2.VideoCapture(vid).get(cv2.CAP_PROP_FRAME_COUNT))



#@title <b><font color="red" size="+3">←</font><font color="black" size="+3"> Crop face in photos</font></b>

success = False

#@title <b><font color="red" size="+3">←</font><font color="black" size="+3"> Transform image</font></b>

#@markdown *Attention! The image is transformed only visually. Sound from the video is not transferred. In addition, FPS decreases, so to restore it, then execute the next block.*

print('Preparing videos and photos for transformation')
source_images = []
photoname = "C:\\Users\\mahta\\OneDrive\\Documents\\GitHub\\autism_Robot\\src\\deep_fake\\first-order-model-master\\first-order-model-master\\me.jpg"
source_image = imageio.imread(photoname)
source_image = resize(source_image, (256, 256))[..., :3]
source_images.append(source_image)

placeholder_bytes = base64.b64decode('iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mP8/x8AAwMCAO+ip1sAAAAASUVORK5CYII=')
placeholder_image = imageio.imread(placeholder_bytes, '.png')
placeholder_image = resize(placeholder_image, (256, 256))[..., :3]



driving_video = imageio.mimread(vid, memtest=False)
driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]

def display(source, driving, generated=None):
  fig = plt.figure(figsize=(8 + 4 * (generated is not None), 6))
  ims = []
  for i in range(len(driving)):
    cols = [[placeholder_image], []]
    for sourceitem in source:
      cols[0].append(sourceitem)
    cols[1].append(driving[i])
    if generated is not None:
      for generateditem in generated:
        cols[1].append(generateditem[i])

    endcols = []
    for thiscol in cols:
      endcols.append(np.concatenate(thiscol, axis=1))
    
    im = plt.imshow(np.vstack(endcols), animated=True) # np.concatenate(cols[0], axis=1)
    plt.axis('off')
    ims.append([im])
  ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)
  plt.close()
  return ani

generator, kp_detector = load_checkpoints(config_path='C:\\Users\\mahta\\OneDrive\\Documents\\GitHub\\autism_Robot\\src\\deep_fake\\first-order-model-master\\first-order-model-master\\config\\vox-256.yaml', 
                                          checkpoint_path='C:\\Users\\mahta\\OneDrive\\Documents\\GitHub\\autism_Robot\\src\\deep_fake\\first-order-model-master\\first-order-model-master\\vox-adv-cpk.pth.tar')

#clear_output()
print('Start the transformation')
videolist = []
predictionlist = []
i = 0
for source_img in source_images:
  videoname = 'result-' + str(i) + '.mp4'
  print('Generating ' + videoname)
  predictions = make_animation(source_img, driving_video, generator, kp_detector, relative=True)
  imageio.mimsave('C:/Users/mahta/OneDrive/Documents/GitHub/autism_Robot/src/deep_fake/first-order-model-master/first-order-model-master/video/intermediate/' + videoname, [img_as_ubyte(frame) for frame in predictions])
  
  videolist.append(videoname)
  predictionlist.append(predictions)
  i += 1
for frame in predictions:
  cv2.imshow("result", img_as_ubyte(frame))
print('Videos generated. Wait a few seconds...')
