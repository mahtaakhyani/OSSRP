<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.6"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>My Project: &lt;a title=&quot;Activity Recognition&quot; href=&quot;https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition&quot; &gt; LSTMs for Human Activity Recognition&lt;/a&gt;</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">My Project
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.6 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

</div><!-- top -->
<div><div class="header">
  <div class="headertitle"><div class="title"><a href="https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition" title="Activity Recognition">LSTMs for Human Activity Recognition</a> </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Human Activity Recognition (HAR) using smartphones dataset and an LSTM RNN. Classifying the type of movement amongst six categories:</p><ul>
<li>WALKING,</li>
<li>WALKING_UPSTAIRS,</li>
<li>WALKING_DOWNSTAIRS,</li>
<li>SITTING,</li>
<li>STANDING,</li>
<li>LAYING.</li>
</ul>
<p>Compared to a classical approach, using a Recurrent Neural Networks (RNN) with Long Short-Term Memory cells (LSTMs) require no or almost no feature engineering. Data can be fed directly into the neural network who acts like a black box, modeling the problem correctly. <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.names">Other research</a> on the activity recognition dataset can use a big amount of feature engineering, which is rather a signal processing approach combined with classical data science techniques. The approach here is rather very simple in terms of how much was the data preprocessed.</p>
<p>Let's use Google's neat Deep Learning library, TensorFlow, demonstrating the usage of an LSTM, a type of Artificial Neural Network that can process sequential data / time series.</p>
<h1><a class="anchor" id="autotoc_md2835"></a>
Video dataset overview</h1>
<p>Follow this link to see a video of the 6 activities recorded in the experiment with one of the participants:</p>
<p><a href="http://www.youtube.com/watch?feature=player_embedded&amp;v=XOEN9W05_4A
" target="_blank"><img src="http://img.youtube.com/vi/XOEN9W05_4A/0.jpg" alt="Video of the experiment" width="400" height="300" border="10" class="inline"/></a> <a href="https://youtu.be/XOEN9W05_4A"><center>[Watch video]</center></a> </p>
<h1><a class="anchor" id="autotoc_md2836"></a>
Details about the input data</h1>
<p>I will be using an LSTM on the data to learn (as a cellphone attached on the waist) to recognise the type of activity that the user is doing. The dataset's description goes like this:</p>
<blockquote class="doxtable">
<p>&zwj;The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. </p>
</blockquote>
<p>That said, I will use the almost raw data: only the gravity effect has been filtered out of the accelerometer as a preprocessing step for another 3D feature as an input to help learning. If you'd ever want to extract the gravity by yourself, you could fork my code on using a <a href="https://github.com/guillaume-chevalier/filtering-stft-and-laplace-transform">Butterworth Low-Pass Filter (LPF) in Python</a> and edit it to have the right cutoff frequency of 0.3 Hz which is a good frequency for activity recognition from body sensors.</p>
<h1><a class="anchor" id="autotoc_md2837"></a>
What is an RNN?</h1>
<p>As explained in <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">this article</a>, an RNN takes many input vectors to process them and output other vectors. It can be roughly pictured like in the image below, imagining each rectangle has a vectorial depth and other special hidden quirks in the image below. <b>In our case, the "many to one" architecture is used</b>: we accept time series of <a href="https://www.quora.com/What-do-samples-features-time-steps-mean-in-LSTM/answer/Guillaume-Chevalier-2">feature vectors</a> (one vector per <a href="https://www.quora.com/What-do-samples-features-time-steps-mean-in-LSTM/answer/Guillaume-Chevalier-2">time step</a>) to convert them to a probability vector at the output for classification. Note that a "one to one" architecture would be a standard feedforward neural network.</p>
<blockquote class="doxtable">
<p>&zwj;<a href="https://www.dl-rnn-course.neuraxio.com/start?utm_source=github_lstm"><img src="https://raw.githubusercontent.com/Neuraxio/Machine-Learning-Figures/master/rnn-architectures.png" alt="RNN Architectures" class="inline"/></a> <a href="https://www.dl-rnn-course.neuraxio.com/start?utm_source=github_lstm">Learn more on RNNs</a> </p>
</blockquote>
<h1><a class="anchor" id="autotoc_md2838"></a>
What is an LSTM?</h1>
<p>An LSTM is an improved RNN. It is more complex, but easier to train, avoiding what is called the vanishing gradient problem. I recommend <a href="https://www.dl-rnn-course.neuraxio.com/start?utm_source=github_lstm">this course</a> for you to learn more on LSTMs.</p>
<blockquote class="doxtable">
<p>&zwj;<a href="https://www.dl-rnn-course.neuraxio.com/start?utm_source=github_lstm">Learn more on LSTMs</a> </p>
</blockquote>
<h1><a class="anchor" id="autotoc_md2839"></a>
Results</h1>
<p>Scroll on! Nice visuals awaits.</p>
<div class="fragment"><div class="line"><span class="comment"># All Includes</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line"><span class="keyword">import</span> matplotlib</div>
<div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div>
<div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf  <span class="comment"># Version 1.0.0 (some previous versions are used in past commits)</span></div>
<div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</div>
<div class="line"> </div>
<div class="line"><span class="keyword">import</span> os</div>
</div><!-- fragment --><div class="fragment"><div class="line"><span class="comment"># Useful Constants</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># Those are separate normalised input features for the neural network</span></div>
<div class="line">INPUT_SIGNAL_TYPES = [</div>
<div class="line">    <span class="stringliteral">&quot;body_acc_x_&quot;</span>,</div>
<div class="line">    <span class="stringliteral">&quot;body_acc_y_&quot;</span>,</div>
<div class="line">    <span class="stringliteral">&quot;body_acc_z_&quot;</span>,</div>
<div class="line">    <span class="stringliteral">&quot;body_gyro_x_&quot;</span>,</div>
<div class="line">    <span class="stringliteral">&quot;body_gyro_y_&quot;</span>,</div>
<div class="line">    <span class="stringliteral">&quot;body_gyro_z_&quot;</span>,</div>
<div class="line">    <span class="stringliteral">&quot;total_acc_x_&quot;</span>,</div>
<div class="line">    <span class="stringliteral">&quot;total_acc_y_&quot;</span>,</div>
<div class="line">    <span class="stringliteral">&quot;total_acc_z_&quot;</span></div>
<div class="line">]</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Output classes to learn how to classify</span></div>
<div class="line">LABELS = [</div>
<div class="line">    <span class="stringliteral">&quot;WALKING&quot;</span>,</div>
<div class="line">    <span class="stringliteral">&quot;WALKING_UPSTAIRS&quot;</span>,</div>
<div class="line">    <span class="stringliteral">&quot;WALKING_DOWNSTAIRS&quot;</span>,</div>
<div class="line">    <span class="stringliteral">&quot;SITTING&quot;</span>,</div>
<div class="line">    <span class="stringliteral">&quot;STANDING&quot;</span>,</div>
<div class="line">    <span class="stringliteral">&quot;LAYING&quot;</span></div>
<div class="line">]</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md2840"></a>
Let's start by downloading the data:</h1>
<div class="fragment"><div class="line"><span class="comment"># Note: Linux bash commands start with a &quot;!&quot; inside those &quot;ipython notebook&quot; cells</span></div>
<div class="line"> </div>
<div class="line">DATA_PATH = <span class="stringliteral">&quot;data/&quot;</span></div>
<div class="line"> </div>
<div class="line">!pwd &amp;&amp; ls</div>
<div class="line">os.chdir(DATA_PATH)</div>
<div class="line">!pwd &amp;&amp; ls</div>
<div class="line"> </div>
<div class="line">!python download_dataset.py</div>
<div class="line"> </div>
<div class="line">!pwd &amp;&amp; ls</div>
<div class="line">os.chdir(<span class="stringliteral">&quot;..&quot;</span>)</div>
<div class="line">!pwd &amp;&amp; ls</div>
<div class="line"> </div>
<div class="line">DATASET_PATH = DATA_PATH + <span class="stringliteral">&quot;UCI HAR Dataset/&quot;</span></div>
<div class="line">print(<span class="stringliteral">&quot;\n&quot;</span> + <span class="stringliteral">&quot;Dataset is now located at: &quot;</span> + DATASET_PATH)</div>
</div><!-- fragment --> <pre class="fragment">/home/ubuntu/pynb/LSTM-Human-Activity-Recognition
data     LSTM_files  LSTM_OLD.ipynb  README.md
LICENSE  LSTM.ipynb  lstm.py         screenlog.0
/home/ubuntu/pynb/LSTM-Human-Activity-Recognition/data
download_dataset.py  source.txt

Downloading...
--2017-05-24 01:49:53--  https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip
Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249
Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 60999314 (58M) [application/zip]
Saving to: ‘UCI HAR Dataset.zip’

100%[======================================&gt;] 60,999,314  1.69MB/s   in 38s    

2017-05-24 01:50:31 (1.55 MB/s) - ‘UCI HAR Dataset.zip’ saved [60999314/60999314]

Downloading done.

Extracting...
Extracting successfully done to /home/ubuntu/pynb/LSTM-Human-Activity-Recognition/data/UCI HAR Dataset.
/home/ubuntu/pynb/LSTM-Human-Activity-Recognition/data
download_dataset.py  __MACOSX  source.txt  UCI HAR Dataset  UCI HAR Dataset.zip
/home/ubuntu/pynb/LSTM-Human-Activity-Recognition
data     LSTM_files  LSTM_OLD.ipynb  README.md
LICENSE  LSTM.ipynb  lstm.py         screenlog.0

Dataset is now located at: data/UCI HAR Dataset/
</pre><h1><a class="anchor" id="autotoc_md2841"></a>
Preparing dataset:</h1>
<div class="fragment"><div class="line">TRAIN = <span class="stringliteral">&quot;train/&quot;</span></div>
<div class="line">TEST = <span class="stringliteral">&quot;test/&quot;</span></div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="comment"># Load &quot;X&quot; (the neural network&#39;s training and testing inputs)</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">def </span>load_X(X_signals_paths):</div>
<div class="line">    X_signals = []</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">for</span> signal_type_path <span class="keywordflow">in</span> X_signals_paths:</div>
<div class="line">        file = open(signal_type_path, <span class="stringliteral">&#39;r&#39;</span>)</div>
<div class="line">        <span class="comment"># Read dataset from disk, dealing with text files&#39; syntax</span></div>
<div class="line">        X_signals.append(</div>
<div class="line">            [np.array(serie, dtype=np.float32) <span class="keywordflow">for</span> serie <span class="keywordflow">in</span> [</div>
<div class="line">                row.replace(<span class="stringliteral">&#39;  &#39;</span>, <span class="stringliteral">&#39; &#39;</span>).strip().split(<span class="stringliteral">&#39; &#39;</span>) <span class="keywordflow">for</span> row <span class="keywordflow">in</span> file</div>
<div class="line">            ]]</div>
<div class="line">        )</div>
<div class="line">        file.close()</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">return</span> np.transpose(np.array(X_signals), (1, 2, 0))</div>
<div class="line"> </div>
<div class="line">X_train_signals_paths = [</div>
<div class="line">    DATASET_PATH + TRAIN + <span class="stringliteral">&quot;Inertial Signals/&quot;</span> + signal + <span class="stringliteral">&quot;train.txt&quot;</span> <span class="keywordflow">for</span> signal <span class="keywordflow">in</span> INPUT_SIGNAL_TYPES</div>
<div class="line">]</div>
<div class="line">X_test_signals_paths = [</div>
<div class="line">    DATASET_PATH + TEST + <span class="stringliteral">&quot;Inertial Signals/&quot;</span> + signal + <span class="stringliteral">&quot;test.txt&quot;</span> <span class="keywordflow">for</span> signal <span class="keywordflow">in</span> INPUT_SIGNAL_TYPES</div>
<div class="line">]</div>
<div class="line"> </div>
<div class="line">X_train = load_X(X_train_signals_paths)</div>
<div class="line">X_test = load_X(X_test_signals_paths)</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="comment"># Load &quot;y&quot; (the neural network&#39;s training and testing outputs)</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">def </span>load_y(y_path):</div>
<div class="line">    file = open(y_path, <span class="stringliteral">&#39;r&#39;</span>)</div>
<div class="line">    <span class="comment"># Read dataset from disk, dealing with text file&#39;s syntax</span></div>
<div class="line">    y_ = np.array(</div>
<div class="line">        [elem <span class="keywordflow">for</span> elem <span class="keywordflow">in</span> [</div>
<div class="line">            row.replace(<span class="stringliteral">&#39;  &#39;</span>, <span class="stringliteral">&#39; &#39;</span>).strip().split(<span class="stringliteral">&#39; &#39;</span>) <span class="keywordflow">for</span> row <span class="keywordflow">in</span> file</div>
<div class="line">        ]],</div>
<div class="line">        dtype=np.int32</div>
<div class="line">    )</div>
<div class="line">    file.close()</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># Substract 1 to each output class for friendly 0-based indexing</span></div>
<div class="line">    <span class="keywordflow">return</span> y_ - 1</div>
<div class="line"> </div>
<div class="line">y_train_path = DATASET_PATH + TRAIN + <span class="stringliteral">&quot;y_train.txt&quot;</span></div>
<div class="line">y_test_path = DATASET_PATH + TEST + <span class="stringliteral">&quot;y_test.txt&quot;</span></div>
<div class="line"> </div>
<div class="line">y_train = load_y(y_train_path)</div>
<div class="line">y_test = load_y(y_test_path)</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md2842"></a>
Additionnal Parameters:</h1>
<p>Here are some core parameter definitions for the training.</p>
<p>For example, the whole neural network's structure could be summarised by enumerating those parameters and the fact that two LSTM are used one on top of another (stacked) output-to-input as hidden layers through time steps.</p>
<div class="fragment"><div class="line"><span class="comment"># Input Data</span></div>
<div class="line"> </div>
<div class="line">training_data_count = len(X_train)  <span class="comment"># 7352 training series (with 50% overlap between each serie)</span></div>
<div class="line">test_data_count = len(X_test)  <span class="comment"># 2947 testing series</span></div>
<div class="line">n_steps = len(X_train[0])  <span class="comment"># 128 timesteps per series</span></div>
<div class="line">n_input = len(X_train[0][0])  <span class="comment"># 9 input parameters per timestep</span></div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="comment"># LSTM Neural Network&#39;s internal structure</span></div>
<div class="line"> </div>
<div class="line">n_hidden = 32 <span class="comment"># Hidden layer num of features</span></div>
<div class="line">n_classes = 6 <span class="comment"># Total classes (should go up, or should go down)</span></div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="comment"># Training</span></div>
<div class="line"> </div>
<div class="line">learning_rate = 0.0025</div>
<div class="line">lambda_loss_amount = 0.0015</div>
<div class="line">training_iters = training_data_count * 300  <span class="comment"># Loop 300 times on the dataset</span></div>
<div class="line">batch_size = 1500</div>
<div class="line">display_iter = 30000  <span class="comment"># To show test set accuracy during training</span></div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="comment"># Some debugging info</span></div>
<div class="line"> </div>
<div class="line">print(<span class="stringliteral">&quot;Some useful info to get an insight on dataset&#39;s shape and normalisation:&quot;</span>)</div>
<div class="line">print(<span class="stringliteral">&quot;(X shape, y shape, every X&#39;s mean, every X&#39;s standard deviation)&quot;</span>)</div>
<div class="line">print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))</div>
<div class="line">print(<span class="stringliteral">&quot;The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.&quot;</span>)</div>
</div><!-- fragment --> <pre class="fragment">Some useful info to get an insight on dataset's shape and normalisation:
(X shape, y shape, every X's mean, every X's standard deviation)
(2947, 128, 9) (2947, 1) 0.0991399 0.395671
The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.
</pre><h1><a class="anchor" id="autotoc_md2843"></a>
Utility functions for training:</h1>
<div class="fragment"><div class="line"><span class="keyword">def </span>LSTM_RNN(_X, _weights, _biases):</div>
<div class="line">    <span class="comment"># Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters.</span></div>
<div class="line">    <span class="comment"># Moreover, two LSTM cells are stacked which adds deepness to the neural network.</span></div>
<div class="line">    <span class="comment"># Note, some code of this notebook is inspired from an slightly different</span></div>
<div class="line">    <span class="comment"># RNN architecture used on another dataset, some of the credits goes to</span></div>
<div class="line">    <span class="comment"># &quot;aymericdamien&quot; under the MIT license.</span></div>
<div class="line"> </div>
<div class="line">    <span class="comment"># (NOTE: This step could be greatly optimised by shaping the dataset once</span></div>
<div class="line">    <span class="comment"># input shape: (batch_size, n_steps, n_input)</span></div>
<div class="line">    _X = tf.transpose(_X, [1, 0, 2])  <span class="comment"># permute n_steps and batch_size</span></div>
<div class="line">    <span class="comment"># Reshape to prepare input to hidden activation</span></div>
<div class="line">    _X = tf.reshape(_X, [-1, n_input])</div>
<div class="line">    <span class="comment"># new shape: (n_steps*batch_size, n_input)</span></div>
<div class="line"> </div>
<div class="line">    <span class="comment"># ReLU activation, thanks to Yu Zhao for adding this improvement here:</span></div>
<div class="line">    _X = tf.nn.relu(tf.matmul(_X, _weights[<span class="stringliteral">&#39;hidden&#39;</span>]) + _biases[<span class="stringliteral">&#39;hidden&#39;</span>])</div>
<div class="line">    <span class="comment"># Split data because rnn cell needs a list of inputs for the RNN inner loop</span></div>
<div class="line">    _X = tf.split(_X, n_steps, 0)</div>
<div class="line">    <span class="comment"># new shape: n_steps * (batch_size, n_hidden)</span></div>
<div class="line"> </div>
<div class="line">    <span class="comment"># Define two stacked LSTM cells (two recurrent layers deep) with tensorflow</span></div>
<div class="line">    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=<span class="keyword">True</span>)</div>
<div class="line">    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=<span class="keyword">True</span>)</div>
<div class="line">    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=<span class="keyword">True</span>)</div>
<div class="line">    <span class="comment"># Get LSTM cell output</span></div>
<div class="line">    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># Get last time step&#39;s output feature for a &quot;many-to-one&quot; style classifier,</span></div>
<div class="line">    <span class="comment"># as in the image describing RNNs at the top of this page</span></div>
<div class="line">    lstm_last_output = outputs[-1]</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># Linear activation</span></div>
<div class="line">    <span class="keywordflow">return</span> tf.matmul(lstm_last_output, _weights[<span class="stringliteral">&#39;out&#39;</span>]) + _biases[<span class="stringliteral">&#39;out&#39;</span>]</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keyword">def </span>extract_batch_size(_train, step, batch_size):</div>
<div class="line">    <span class="comment"># Function to fetch a &quot;batch_size&quot; amount of data from &quot;(X|y)_train&quot; data.</span></div>
<div class="line"> </div>
<div class="line">    shape = list(_train.shape)</div>
<div class="line">    shape[0] = batch_size</div>
<div class="line">    batch_s = np.empty(shape)</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(batch_size):</div>
<div class="line">        <span class="comment"># Loop index</span></div>
<div class="line">        index = ((step-1)*batch_size + i) % len(_train)</div>
<div class="line">        batch_s[i] = _train[index]</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">return</span> batch_s</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keyword">def </span>one_hot(y_, n_classes=n_classes):</div>
<div class="line">    <span class="comment"># Function to encode neural one-hot output labels from number indexes</span></div>
<div class="line">    <span class="comment"># e.g.:</span></div>
<div class="line">    <span class="comment"># one_hot(y_=[[5], [0], [3]], n_classes=6):</span></div>
<div class="line">    <span class="comment">#     return [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]</span></div>
<div class="line"> </div>
<div class="line">    y_ = y_.reshape(len(y_))</div>
<div class="line">    <span class="keywordflow">return</span> np.eye(n_classes)[np.array(y_, dtype=np.int32)]  <span class="comment"># Returns FLOATS</span></div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md2844"></a>
Let's get serious and build the neural network:</h1>
<div class="fragment"><div class="line"><span class="comment"># Graph input/output</span></div>
<div class="line">x = tf.placeholder(tf.float32, [<span class="keywordtype">None</span>, n_steps, n_input])</div>
<div class="line">y = tf.placeholder(tf.float32, [<span class="keywordtype">None</span>, n_classes])</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Graph weights</span></div>
<div class="line">weights = {</div>
<div class="line">    <span class="stringliteral">&#39;hidden&#39;</span>: tf.Variable(tf.random_normal([n_input, n_hidden])), <span class="comment"># Hidden layer weights</span></div>
<div class="line">    <span class="stringliteral">&#39;out&#39;</span>: tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))</div>
<div class="line">}</div>
<div class="line">biases = {</div>
<div class="line">    <span class="stringliteral">&#39;hidden&#39;</span>: tf.Variable(tf.random_normal([n_hidden])),</div>
<div class="line">    <span class="stringliteral">&#39;out&#39;</span>: tf.Variable(tf.random_normal([n_classes]))</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line">pred = LSTM_RNN(x, weights, biases)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Loss, optimizer and evaluation</span></div>
<div class="line">l2 = lambda_loss_amount * sum(</div>
<div class="line">    tf.nn.l2_loss(tf_var) <span class="keywordflow">for</span> tf_var <span class="keywordflow">in</span> tf.trainable_variables()</div>
<div class="line">) <span class="comment"># L2 loss prevents this overkill neural network to overfit the data</span></div>
<div class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 <span class="comment"># Softmax loss</span></div>
<div class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) <span class="comment"># Adam Optimizer</span></div>
<div class="line"> </div>
<div class="line">correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))</div>
<div class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md2845"></a>
Hooray, now train the neural network:</h1>
<div class="fragment"><div class="line"><span class="comment"># To keep track of training&#39;s performance</span></div>
<div class="line">test_losses = []</div>
<div class="line">test_accuracies = []</div>
<div class="line">train_losses = []</div>
<div class="line">train_accuracies = []</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Launch the graph</span></div>
<div class="line">sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=<span class="keyword">True</span>))</div>
<div class="line">init = tf.global_variables_initializer()</div>
<div class="line">sess.run(init)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Perform Training steps with &quot;batch_size&quot; amount of example data at each loop</span></div>
<div class="line">step = 1</div>
<div class="line"><span class="keywordflow">while</span> step * batch_size &lt;= training_iters:</div>
<div class="line">    batch_xs =         extract_batch_size(X_train, step, batch_size)</div>
<div class="line">    batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># Fit training using batch data</span></div>
<div class="line">    _, loss, acc = sess.run(</div>
<div class="line">        [optimizer, cost, accuracy],</div>
<div class="line">        feed_dict={</div>
<div class="line">            x: batch_xs,</div>
<div class="line">            y: batch_ys</div>
<div class="line">        }</div>
<div class="line">    )</div>
<div class="line">    train_losses.append(loss)</div>
<div class="line">    train_accuracies.append(acc)</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># Evaluate network only at some steps for faster training:</span></div>
<div class="line">    <span class="keywordflow">if</span> (step*batch_size % display_iter == 0) <span class="keywordflow">or</span> (step == 1) <span class="keywordflow">or</span> (step * batch_size &gt; training_iters):</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># To not spam console, show training accuracy/loss in this &quot;if&quot;</span></div>
<div class="line">        print(<span class="stringliteral">&quot;Training iter #&quot;</span> + str(step*batch_size) + \</div>
<div class="line">              <span class="stringliteral">&quot;:   Batch Loss = &quot;</span> + <span class="stringliteral">&quot;{:.6f}&quot;</span>.format(loss) + \</div>
<div class="line">              <span class="stringliteral">&quot;, Accuracy = {}&quot;</span>.format(acc))</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Evaluation on the test set (no learning made here - just evaluation for diagnosis)</span></div>
<div class="line">        loss, acc = sess.run(</div>
<div class="line">            [cost, accuracy],</div>
<div class="line">            feed_dict={</div>
<div class="line">                x: X_test,</div>
<div class="line">                y: one_hot(y_test)</div>
<div class="line">            }</div>
<div class="line">        )</div>
<div class="line">        test_losses.append(loss)</div>
<div class="line">        test_accuracies.append(acc)</div>
<div class="line">        print(<span class="stringliteral">&quot;PERFORMANCE ON TEST SET: &quot;</span> + \</div>
<div class="line">              <span class="stringliteral">&quot;Batch Loss = {}&quot;</span>.format(loss) + \</div>
<div class="line">              <span class="stringliteral">&quot;, Accuracy = {}&quot;</span>.format(acc))</div>
<div class="line"> </div>
<div class="line">    step += 1</div>
<div class="line"> </div>
<div class="line">print(<span class="stringliteral">&quot;Optimization Finished!&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Accuracy for test data</span></div>
<div class="line"> </div>
<div class="line">one_hot_predictions, accuracy, final_loss = sess.run(</div>
<div class="line">    [pred, accuracy, cost],</div>
<div class="line">    feed_dict={</div>
<div class="line">        x: X_test,</div>
<div class="line">        y: one_hot(y_test)</div>
<div class="line">    }</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line">test_losses.append(final_loss)</div>
<div class="line">test_accuracies.append(accuracy)</div>
<div class="line"> </div>
<div class="line">print(<span class="stringliteral">&quot;FINAL RESULT: &quot;</span> + \</div>
<div class="line">      <span class="stringliteral">&quot;Batch Loss = {}&quot;</span>.format(final_loss) + \</div>
<div class="line">      <span class="stringliteral">&quot;, Accuracy = {}&quot;</span>.format(accuracy))</div>
</div><!-- fragment --> <pre class="fragment">WARNING:tensorflow:From &lt;ipython-input-19-3339689e51f6&gt;:9: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Training iter #1500:   Batch Loss = 5.416760, Accuracy = 0.15266665816307068
PERFORMANCE ON TEST SET: Batch Loss = 4.880829811096191, Accuracy = 0.05632847175002098
Training iter #30000:   Batch Loss = 3.031930, Accuracy = 0.607333242893219
PERFORMANCE ON TEST SET: Batch Loss = 3.0515167713165283, Accuracy = 0.6067186594009399
Training iter #60000:   Batch Loss = 2.672764, Accuracy = 0.7386666536331177
PERFORMANCE ON TEST SET: Batch Loss = 2.780435085296631, Accuracy = 0.7027485370635986
Training iter #90000:   Batch Loss = 2.378301, Accuracy = 0.8366667032241821
PERFORMANCE ON TEST SET: Batch Loss = 2.6019773483276367, Accuracy = 0.7617915868759155
Training iter #120000:   Batch Loss = 2.127290, Accuracy = 0.9066667556762695
PERFORMANCE ON TEST SET: Batch Loss = 2.3625404834747314, Accuracy = 0.8116728663444519
Training iter #150000:   Batch Loss = 1.929805, Accuracy = 0.9380000233650208
PERFORMANCE ON TEST SET: Batch Loss = 2.306251049041748, Accuracy = 0.8276212215423584
Training iter #180000:   Batch Loss = 1.971904, Accuracy = 0.9153333902359009
PERFORMANCE ON TEST SET: Batch Loss = 2.0835530757904053, Accuracy = 0.8771631121635437
Training iter #210000:   Batch Loss = 1.860249, Accuracy = 0.8613333702087402
PERFORMANCE ON TEST SET: Batch Loss = 1.9994492530822754, Accuracy = 0.8788597583770752
Training iter #240000:   Batch Loss = 1.626292, Accuracy = 0.9380000233650208
PERFORMANCE ON TEST SET: Batch Loss = 1.879166603088379, Accuracy = 0.8944689035415649
Training iter #270000:   Batch Loss = 1.582758, Accuracy = 0.9386667013168335
PERFORMANCE ON TEST SET: Batch Loss = 2.0341007709503174, Accuracy = 0.8361043930053711
Training iter #300000:   Batch Loss = 1.620352, Accuracy = 0.9306666851043701
PERFORMANCE ON TEST SET: Batch Loss = 1.8185184001922607, Accuracy = 0.8639293313026428
Training iter #330000:   Batch Loss = 1.474394, Accuracy = 0.9693333506584167
PERFORMANCE ON TEST SET: Batch Loss = 1.7638503313064575, Accuracy = 0.8747878670692444
Training iter #360000:   Batch Loss = 1.406998, Accuracy = 0.9420000314712524
PERFORMANCE ON TEST SET: Batch Loss = 1.5946787595748901, Accuracy = 0.902273416519165
Training iter #390000:   Batch Loss = 1.362515, Accuracy = 0.940000057220459
PERFORMANCE ON TEST SET: Batch Loss = 1.5285792350769043, Accuracy = 0.9046487212181091
Training iter #420000:   Batch Loss = 1.252860, Accuracy = 0.9566667079925537
PERFORMANCE ON TEST SET: Batch Loss = 1.4635565280914307, Accuracy = 0.9107565879821777
Training iter #450000:   Batch Loss = 1.190078, Accuracy = 0.9553333520889282
...
PERFORMANCE ON TEST SET: Batch Loss = 0.42567864060401917, Accuracy = 0.9324736595153809
Training iter #2070000:   Batch Loss = 0.342763, Accuracy = 0.9326667189598083
PERFORMANCE ON TEST SET: Batch Loss = 0.4292983412742615, Accuracy = 0.9273836612701416
Training iter #2100000:   Batch Loss = 0.259442, Accuracy = 0.9873334169387817
PERFORMANCE ON TEST SET: Batch Loss = 0.44131210446357727, Accuracy = 0.9273836612701416
Training iter #2130000:   Batch Loss = 0.284630, Accuracy = 0.9593333601951599
PERFORMANCE ON TEST SET: Batch Loss = 0.46982717514038086, Accuracy = 0.9093992710113525
Training iter #2160000:   Batch Loss = 0.299012, Accuracy = 0.9686667323112488
PERFORMANCE ON TEST SET: Batch Loss = 0.48389002680778503, Accuracy = 0.9138105511665344
Training iter #2190000:   Batch Loss = 0.287106, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.4670214056968689, Accuracy = 0.9216151237487793
Optimization Finished!
FINAL RESULT: Batch Loss = 0.45611169934272766, Accuracy = 0.9165252447128296
</pre><h1><a class="anchor" id="autotoc_md2846"></a>
Training is good, but having visual insight is even better:</h1>
<p>Okay, let's plot this simply in the notebook for now.</p>
<div class="fragment"><div class="line"><span class="comment"># (Inline plots: )</span></div>
<div class="line">%matplotlib inline</div>
<div class="line"> </div>
<div class="line">font = {</div>
<div class="line">    <span class="stringliteral">&#39;family&#39;</span> : <span class="stringliteral">&#39;Bitstream Vera Sans&#39;</span>,</div>
<div class="line">    <span class="stringliteral">&#39;weight&#39;</span> : <span class="stringliteral">&#39;bold&#39;</span>,</div>
<div class="line">    <span class="stringliteral">&#39;size&#39;</span>   : 18</div>
<div class="line">}</div>
<div class="line">matplotlib.rc(<span class="stringliteral">&#39;font&#39;</span>, **font)</div>
<div class="line"> </div>
<div class="line">width = 12</div>
<div class="line">height = 12</div>
<div class="line">plt.figure(figsize=(width, height))</div>
<div class="line"> </div>
<div class="line">indep_train_axis = np.array(range(batch_size, (len(train_losses)+1)*batch_size, batch_size))</div>
<div class="line">plt.plot(indep_train_axis, np.array(train_losses),     <span class="stringliteral">&quot;b--&quot;</span>, label=<span class="stringliteral">&quot;Train losses&quot;</span>)</div>
<div class="line">plt.plot(indep_train_axis, np.array(train_accuracies), <span class="stringliteral">&quot;g--&quot;</span>, label=<span class="stringliteral">&quot;Train accuracies&quot;</span>)</div>
<div class="line"> </div>
<div class="line">indep_test_axis = np.append(</div>
<div class="line">    np.array(range(batch_size, len(test_losses)*display_iter, display_iter)[:-1]),</div>
<div class="line">    [training_iters]</div>
<div class="line">)</div>
<div class="line">plt.plot(indep_test_axis, np.array(test_losses),     <span class="stringliteral">&quot;b-&quot;</span>, label=<span class="stringliteral">&quot;Test losses&quot;</span>)</div>
<div class="line">plt.plot(indep_test_axis, np.array(test_accuracies), <span class="stringliteral">&quot;g-&quot;</span>, label=<span class="stringliteral">&quot;Test accuracies&quot;</span>)</div>
<div class="line"> </div>
<div class="line">plt.title(<span class="stringliteral">&quot;Training session&#39;s progress over iterations&quot;</span>)</div>
<div class="line">plt.legend(loc=<span class="stringliteral">&#39;upper right&#39;</span>, shadow=<span class="keyword">True</span>)</div>
<div class="line">plt.ylabel(<span class="stringliteral">&#39;Training Progress (Loss or Accuracy values)&#39;</span>)</div>
<div class="line">plt.xlabel(<span class="stringliteral">&#39;Training iteration&#39;</span>)</div>
<div class="line"> </div>
<div class="line">plt.show()</div>
</div><!-- fragment --><p><img src="LSTM_files/LSTM_16_0.png" alt="LSTM Training Testing Comparison Curve" class="inline"/></p>
<h1><a class="anchor" id="autotoc_md2847"></a>
And finally, the multi-class confusion matrix and metrics!</h1>
<div class="fragment"><div class="line"><span class="comment"># Results</span></div>
<div class="line"> </div>
<div class="line">predictions = one_hot_predictions.argmax(1)</div>
<div class="line"> </div>
<div class="line">print(<span class="stringliteral">&quot;Testing Accuracy: {}%&quot;</span>.format(100*accuracy))</div>
<div class="line"> </div>
<div class="line">print(<span class="stringliteral">&quot;&quot;</span>)</div>
<div class="line">print(<span class="stringliteral">&quot;Precision: {}%&quot;</span>.format(100*metrics.precision_score(y_test, predictions, average=<span class="stringliteral">&quot;weighted&quot;</span>)))</div>
<div class="line">print(<span class="stringliteral">&quot;Recall: {}%&quot;</span>.format(100*metrics.recall_score(y_test, predictions, average=<span class="stringliteral">&quot;weighted&quot;</span>)))</div>
<div class="line">print(<span class="stringliteral">&quot;f1_score: {}%&quot;</span>.format(100*metrics.f1_score(y_test, predictions, average=<span class="stringliteral">&quot;weighted&quot;</span>)))</div>
<div class="line"> </div>
<div class="line">print(<span class="stringliteral">&quot;&quot;</span>)</div>
<div class="line">print(<span class="stringliteral">&quot;Confusion Matrix:&quot;</span>)</div>
<div class="line">confusion_matrix = metrics.confusion_matrix(y_test, predictions)</div>
<div class="line">print(confusion_matrix)</div>
<div class="line">normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100</div>
<div class="line"> </div>
<div class="line">print(<span class="stringliteral">&quot;&quot;</span>)</div>
<div class="line">print(<span class="stringliteral">&quot;Confusion matrix (normalised to % of total test data):&quot;</span>)</div>
<div class="line">print(normalised_confusion_matrix)</div>
<div class="line">print(<span class="stringliteral">&quot;Note: training and testing data is not equally distributed amongst classes, &quot;</span>)</div>
<div class="line">print(<span class="stringliteral">&quot;so it is normal that more than a 6th of the data is correctly classifier in the last category.&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Plot Results:</span></div>
<div class="line">width = 12</div>
<div class="line">height = 12</div>
<div class="line">plt.figure(figsize=(width, height))</div>
<div class="line">plt.imshow(</div>
<div class="line">    normalised_confusion_matrix,</div>
<div class="line">    interpolation=<span class="stringliteral">&#39;nearest&#39;</span>,</div>
<div class="line">    cmap=plt.cm.rainbow</div>
<div class="line">)</div>
<div class="line">plt.title(<span class="stringliteral">&quot;Confusion matrix \n(normalised to % of total test data)&quot;</span>)</div>
<div class="line">plt.colorbar()</div>
<div class="line">tick_marks = np.arange(n_classes)</div>
<div class="line">plt.xticks(tick_marks, LABELS, rotation=90)</div>
<div class="line">plt.yticks(tick_marks, LABELS)</div>
<div class="line">plt.tight_layout()</div>
<div class="line">plt.ylabel(<span class="stringliteral">&#39;True label&#39;</span>)</div>
<div class="line">plt.xlabel(<span class="stringliteral">&#39;Predicted label&#39;</span>)</div>
<div class="line">plt.show()</div>
</div><!-- fragment --> <pre class="fragment">Testing Accuracy: 91.65252447128296%

Precision: 91.76286479743305%
Recall: 91.65252799457076%
f1_score: 91.6437546304815%

Confusion Matrix:
[[466   2  26   0   2   0]
 [  5 441  25   0   0   0]
 [  1   0 419   0   0   0]
 [  1   1   0 396  87   6]
 [  2   1   0  87 442   0]
 [  0   0   0   0   0 537]]

Confusion matrix (normalised to % of total test data):
[[ 15.81269073   0.06786563   0.88225317   0.           0.06786563   0.        ]
 [  0.16966406  14.96437073   0.84832031   0.           0.           0.        ]
 [  0.03393281   0.          14.21784878   0.           0.           0.        ]
 [  0.03393281   0.03393281   0.          13.43739319   2.95215464
    0.20359688]
 [  0.06786563   0.03393281   0.           2.95215464  14.99830341   0.        ]
 [  0.           0.           0.           0.           0.          18.22192001]]
Note: training and testing data is not equally distributed amongst classes,
so it is normal that more than a 6th of the data is correctly classifier in the last category.
</pre><p><img src="LSTM_files/LSTM_18_1.png" alt="Confusion Matrix" class="inline"/></p>
<div class="fragment"><div class="line">sess.close()</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md2848"></a>
Conclusion</h1>
<p>Outstandingly, <b>the final accuracy is of 91%</b>! And it can peak to values such as 93.25%, at some moments of luck during the training, depending on how the neural network's weights got initialized at the start of the training, randomly.</p>
<p>This means that the neural networks is almost always able to correctly identify the movement type! Remember, the phone is attached on the waist and each series to classify has just a 128 sample window of two internal sensors (a.k.a. 2.56 seconds at 50 FPS), so it amazes me how those predictions are extremely accurate given this small window of context and raw data. I've validated and re-validated that there is no important bug, and the community used and tried this code a lot. (Note: be sure to report something in the issue tab if you find bugs, otherwise <a href="https://www.quora.com/">Quora</a>, <a href="https://stackoverflow.com/questions/tagged/tensorflow?sort=votes&amp;pageSize=50">StackOverflow</a>, and other <a href="https://stackexchange.com/sites#science">StackExchange</a> sites are the places for asking questions.)</p>
<p>I specially did not expect such good results for guessing between the labels "SITTING" and "STANDING". Those are seemingly almost the same thing from the point of view of a device placed at waist level according to how the dataset was originally gathered. Thought, it is still possible to see a little cluster on the matrix between those classes, which drifts away just a bit from the identity. This is great.</p>
<p>It is also possible to see that there was a slight difficulty in doing the difference between "WALKING", "WALKING_UPSTAIRS" and "WALKING_DOWNSTAIRS". Obviously, those activities are quite similar in terms of movements.</p>
<p>I also tried my code without the gyroscope, using only the 3D accelerometer's 6 features (and not changing the training hyperparameters), and got an accuracy of 87%. In general, gyroscopes consumes more power than accelerometers, so it is preferable to turn them off.</p>
<h1><a class="anchor" id="autotoc_md2849"></a>
Improvements</h1>
<p>In <a href="https://github.com/guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs">another open-source repository of mine</a>, the accuracy is pushed up to nearly 94% using a special deep LSTM architecture which combines the concepts of bidirectional RNNs, residual connections, and stacked cells. This architecture is also tested on another similar activity dataset. It resembles the nice architecture used in "[Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf)", without an attention mechanism, and with just the encoder part - as a "many to one" architecture instead of a "many to many" to be adapted to the Human Activity Recognition (HAR) problem. I also worked more on the problem and came up with the <a href="https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network">LARNN</a>, however it's complicated for just a little gain. Thus the current, original activity recognition project is simply better to use for its simplicity. We've also coded a <a href="https://github.com/Neuraxio/Kata-Clean-Machine-Learning-From-Dirty-Code">non-deep learning machine learning pipeline</a> on the same datasets using classical featurization techniques and older machine learning algorithms.</p>
<p>If you want to learn more about deep learning, I have also built a list of the learning ressources for deep learning which have revealed to be the most useful to me <a href="https://github.com/guillaume-chevalier/Awesome-Deep-Learning-Resources">here</a>.</p>
<h1><a class="anchor" id="autotoc_md2850"></a>
References</h1>
<p>The <a href="https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones">dataset</a> can be found on the UCI Machine Learning Repository:</p>
<blockquote class="doxtable">
<p>&zwj;Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013. </p>
</blockquote>
<h1><a class="anchor" id="autotoc_md2851"></a>
Citation</h1>
<p>Copyright (c) 2016 Guillaume Chevalier. To cite my code, you can point to the URL of the GitHub repository, for example:</p>
<blockquote class="doxtable">
<p>&zwj;Guillaume Chevalier, LSTMs for Human Activity Recognition, 2016, <a href="https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition">https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition</a> </p>
</blockquote>
<p>My code is available for free and even for private usage for anyone under the <a href="https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/blob/master/LICENSE">MIT License</a>, however I ask to cite for using the code.</p>
<p>Here is the BibTeX citation code: </p><div class="fragment"><div class="line">@misc{chevalier2016lstms,</div>
<div class="line">  title={LSTMs for human activity recognition},</div>
<div class="line">  author={Chevalier, Guillaume},</div>
<div class="line">  year={2016}</div>
<div class="line">}</div>
</div><!-- fragment --><p>I've also published a second paper, with contributors, regarding a <a href="https://github.com/guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs">second iteration as an improvement of this work</a>, with deeper neural networks. The paper is available on <a href="https://arxiv.org/abs/1708.08989">arXiv</a>. Here is the BibTeX citation code for this newer piece of work based on this project: </p><div class="fragment"><div class="line">@article{DBLP:journals/corr/abs-1708-08989,</div>
<div class="line">  author    = {Yu Zhao and</div>
<div class="line">               Rennong Yang and</div>
<div class="line">               Guillaume Chevalier and</div>
<div class="line">               Maoguo Gong},</div>
<div class="line">  title     = {Deep Residual Bidir-LSTM for Human Activity Recognition Using Wearable</div>
<div class="line">               Sensors},</div>
<div class="line">  journal   = {CoRR},</div>
<div class="line">  volume    = {abs/1708.08989},</div>
<div class="line">  year      = {2017},</div>
<div class="line">  url       = {http://arxiv.org/abs/1708.08989},</div>
<div class="line">  archivePrefix = {arXiv},</div>
<div class="line">  eprint    = {1708.08989},</div>
<div class="line">  timestamp = {Mon, 13 Aug 2018 16:46:48 +0200},</div>
<div class="line">  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-08989},</div>
<div class="line">  bibsource = {dblp computer science bibliography, https://dblp.org}</div>
<div class="line">}</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md2852"></a>
Extra links</h1>
<h2><a class="anchor" id="autotoc_md2853"></a>
Connect with me</h2>
<ul>
<li><a href="https://github.com/guillaume-chevalier/">GitHub</a></li>
<li><a href="https://ca.linkedin.com/in/chevalierg">LinkedIn</a></li>
<li><a href="https://www.youtube.com/c/GuillaumeChevalier">YouTube</a></li>
</ul>
<h2><a class="anchor" id="autotoc_md2854"></a>
Liked this project? Did it help you? Leave a &lt;a href="https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/stargazers" &gt;star&lt;/a&gt;, &lt;a href="https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/network/members" &gt;fork&lt;/a&gt; and share the love!</h2>
<p>This activity recognition project has been seen in:</p>
<ul>
<li><a href="https://news.ycombinator.com/item?id=13049143">Hacker News 1st page</a></li>
<li><a href="https://github.com/jtoy/awesome-tensorflow#tutorials">Awesome TensorFlow</a></li>
<li><a href="https://github.com/astorfi/TensorFlow-World#some-useful-tutorials">TensorFlow World</a></li>
<li>And more.</li>
</ul>
<hr  />
<div class="fragment"><div class="line"><span class="comment"># Let&#39;s convert this notebook to a README automatically for the GitHub project&#39;s title page:</span></div>
<div class="line">!jupyter nbconvert --to markdown LSTM.ipynb</div>
<div class="line">!mv LSTM.md README.md</div>
</div><!-- fragment --> <pre class="fragment">[NbConvertApp] Converting notebook LSTM.ipynb to markdown
[NbConvertApp] Support files will be in LSTM_files/
[NbConvertApp] Making directory LSTM_files
[NbConvertApp] Making directory LSTM_files
[NbConvertApp] Writing 38654 bytes to LSTM.md
</pre> </div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.6
</small></address>
</body>
</html>
