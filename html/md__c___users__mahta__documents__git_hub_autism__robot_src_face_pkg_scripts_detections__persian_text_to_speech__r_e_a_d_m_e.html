<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.6"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>My Project: Persian text to speech</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">My Project
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.6 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

</div><!-- top -->
<div><div class="header">
  <div class="headertitle"><div class="title">Persian text to speech </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><b>A convolutional sequence to sequence model for Persian text to speech based on <a href="https://arxiv.org/abs/1710.08969">Tachibana et al</a> with a few modifications:</b></p>
<p>1) The article didn’t mention position embedding, but in order to give the model a sense of position awareness I added it (it turned out to be useful in the original <a href="https://arxiv.org/abs/1705.03122">convolutional seq2seq paper</a></p>
<p>2) In the paper they trained both networks with a combination of the L1 loss and an additional binary cross entropy loss , they claim it was beneficial, I found it to be an odd choice of loss function.to validate their idea I trained networks with and without binary cross entropy loss , but adding binary cross entropy loss didn’t make much difference.</p>
<p>3) In the original paper they used a fixed learning rate of 0.001, but I decayed it.according to the <a href="https://arxiv.org/abs/1412.6980">ADAM</a> paper Good default settings for the tested machine learning problems are α = 0.001,β1 = 0.9, β2 = 0.999 but in my case it didn't work!(gradients keep exploding)</p>
<p>4) I implemented a standard scaled dot-product attention but it mostly failed to converge. Guided attention is a simple but good idea such that the model converge way faster than a standard scaled dot-product attention. Training the attention part was a bottleneck.</p>
<p>In the following figure a schematic of the model architecture is presented:</p>
<p><img src="/imgs/texttomel.jpg" alt="text to mel" class="inline"/></p>
<p><b>Dataset: (a Persian single speaker speech dataset that last more than 30 hours [narrated by Maryam Mahboub])</b></p>
<p>There aren’t any available datasets for Persian text to speech so I decided to make my own. I chose to use audio books from <a href="www.navaar.ir">navar</a> (I couldn’t find any websites for free public domain audiobooks) then I checked for text availability, if text is not available the audio book is excluded. None of them were available so I $$$$ a few online bookstores like <a href="http://fidibo.com/">Fidibo</a> to get them. All audio files are sampled at 44 kHz which means there are 44100 values stored for every second of audio, too much information, WAY TOO MUCH! So I down sampled the pcm/wav audio samples from 44 kHz to 22 kHz. I also discarded the stereo channel as it contains highly redundant information.</p>
<p>The audio from <a href="www.navaar.ir">navar</a> comes in large files which don’t suit the text to speech task. At first I decided to use <a href="http://linguistics.berkeley.edu/plab/guestwiki/index.php?title=Forced_alignment">automatic force-alignment technique</a> (given an audio clip containing speech [without environmental noise] and the corresponding transcript, computing a forced alignment is the process of determining, for each fragment of the transcript, the time interval containing the spoken text of the fragment) to align the text corpus with audio clips but it didn’t guarantee the correct alignments because texts were slightly different than speech. I finally ended up pragmatically splitting large chunk of audio files into smaller parts by using silence detection and manually aligned text with audio segments.</p>
<p>The distribution of audio lengths for my dataset is shown in the following figure: (90 percentage of the samples have a duration between 2 and 7 seconds)</p>
<p><img src="/imgs/hist.png" alt="text to mel" class="inline"/></p>
<p>To speed up the training speech and reduce CPU usage I first preprocessed all audio files (using Fourier’s Transform to convert our audio data to the frequency domain). Extracting the audio spectrogram on the fly is expensive.</p>
<p>I implemented the models in Tensorflow and they were trained on a single Nvidia GTX 1050Ti with 4GB memory (a batch size of 32 for text to spectrogram and 8 for super resolution network ).</p>
<p>In the following figure the learned character embeddings is shown:</p>
<p><img src="/imgs/char-embedding.jpg" alt="text to mel" class="inline"/></p>
<p><b>Some generated samples:</b>(NOTE:I use the <a href="https://ieeexplore.ieee.org/document/6701851/">Griffin Lim algorithm</a> to invert the prediction back to audio signal.i found that's the main source of audible artifacts)</p>
<p><a href="https://soundcloud.com/12211221212/sets/persian-text-2-speechwoman">Woman-Maryam Mahboub</a></p>
<p><a href="https://soundcloud.com/12211221212/sets/persian-text-2-speech">Man-Arman Soltan zadeh</a></p>
<p><b>Pre-trained model:</b></p>
<p>You can download pre-trained weights from <a href="https://www.dropbox.com/s/48wy3kw4e512ax7/logs.rar?dl=0">here</a></p>
<p><b>Script files</b></p>
<p>if you want to train and test your own datasets: Modify train.ipynb including args and data path</p>
<p>demo.ipynb:Enter your sentences and listen to the generated audio</p>
<p>#TODO:Implement mononotic attention </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.6
</small></address>
</body>
</html>
